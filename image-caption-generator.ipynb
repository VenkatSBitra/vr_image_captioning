{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Step 1:- Import the required libraries \n\n\nimport numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport os\nimport glob\nfrom PIL import Image\nfrom time import time\n\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:07.285964Z","iopub.execute_input":"2022-05-08T06:53:07.286215Z","iopub.status.idle":"2022-05-08T06:53:07.298744Z","shell.execute_reply.started":"2022-05-08T06:53:07.286188Z","shell.execute_reply":"2022-05-08T06:53:07.297963Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the data**","metadata":{}},{"cell_type":"code","source":"## Step 2 : Data Loading and Pre-Processing\n\ntoken_path = \"../input/flickr-8k/Flickr8k.token.txt\"\ntrain_images_path = '../input/flickr-8k/Flickr_8k.trainImages.txt'\ntest_images_path = '../input/flickr-8k/Flickr_8k.testImages.txt'\nimages_path = '../input/flickr8k/Images/'\nglove_path = '../input/glove6b/'\n\ndoc = open(token_path,'r').read()\nprint(doc[:410])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:18:59.185362Z","iopub.execute_input":"2022-05-08T17:18:59.185611Z","iopub.status.idle":"2022-05-08T17:18:59.258508Z","shell.execute_reply.started":"2022-05-08T17:18:59.185583Z","shell.execute_reply":"2022-05-08T17:18:59.257774Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing on the images i.e., collecting vocabulary and descriptions","metadata":{}},{"cell_type":"code","source":"descriptions = dict()\nfor line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) > 2:\n          image_id = tokens[0].split('.')[0]\n          image_desc = ' '.join(tokens[1:])\n          if image_id not in descriptions:\n              descriptions[image_id] = list()\n          descriptions[image_id].append(image_desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:07.321796Z","iopub.execute_input":"2022-05-08T06:53:07.322210Z","iopub.status.idle":"2022-05-08T06:53:07.434179Z","shell.execute_reply.started":"2022-05-08T06:53:07.322173Z","shell.execute_reply":"2022-05-08T06:53:07.433235Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"table = str.maketrans('', '', string.punctuation)\nfor key, desc_list in descriptions.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        desc = desc.split()\n        desc = [word.lower() for word in desc]\n        desc = [w.translate(table) for w in desc]\n        desc_list[i] =  ' '.join(desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:07.435599Z","iopub.execute_input":"2022-05-08T06:53:07.435934Z","iopub.status.idle":"2022-05-08T06:53:07.779766Z","shell.execute_reply.started":"2022-05-08T06:53:07.435895Z","shell.execute_reply":"2022-05-08T06:53:07.779007Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## Lets visualize and example image\npic = '1000268201_693b08cb0e.jpg'\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\ndescriptions['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:07.781619Z","iopub.execute_input":"2022-05-08T06:53:07.781839Z","iopub.status.idle":"2022-05-08T06:53:08.021555Z","shell.execute_reply.started":"2022-05-08T06:53:07.781812Z","shell.execute_reply":"2022-05-08T06:53:08.020875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vocabulary = set()\nfor key in descriptions.keys():\n        [vocabulary.update(d.split()) for d in descriptions[key]]\nprint('Original Vocabulary Size: %d' % len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.022493Z","iopub.execute_input":"2022-05-08T06:53:08.022731Z","iopub.status.idle":"2022-05-08T06:53:08.088005Z","shell.execute_reply.started":"2022-05-08T06:53:08.022700Z","shell.execute_reply":"2022-05-08T06:53:08.087277Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"lines = list()\nfor key, desc_list in descriptions.items():\n    for desc in desc_list:\n        lines.append(key + ' ' + desc)\nnew_descriptions = '\\n'.join(lines)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.089073Z","iopub.execute_input":"2022-05-08T06:53:08.089313Z","iopub.status.idle":"2022-05-08T06:53:08.114978Z","shell.execute_reply.started":"2022-05-08T06:53:08.089272Z","shell.execute_reply":"2022-05-08T06:53:08.114312Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"doc = open(train_images_path,'r').read()\ndataset = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset.append(identifier)\n\ntrain = set(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.116016Z","iopub.execute_input":"2022-05-08T06:53:08.116244Z","iopub.status.idle":"2022-05-08T06:53:08.133777Z","shell.execute_reply.started":"2022-05-08T06:53:08.116211Z","shell.execute_reply":"2022-05-08T06:53:08.133120Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"img = glob.glob(images_path + '*.jpg')\ntrain_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\ntrain_img = []\nfor i in img: \n    if i[len(images_path):] in train_images:\n        train_img.append(i)\n\ntest_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\ntest_img = []\nfor i in img: \n    if i[len(images_path):] in test_images: \n        test_img.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.134899Z","iopub.execute_input":"2022-05-08T06:53:08.135140Z","iopub.status.idle":"2022-05-08T06:53:08.334365Z","shell.execute_reply.started":"2022-05-08T06:53:08.135108Z","shell.execute_reply":"2022-05-08T06:53:08.333660Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in train:\n        if image_id not in train_descriptions:\n            train_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        train_descriptions[image_id].append(desc)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.336181Z","iopub.execute_input":"2022-05-08T06:53:08.336667Z","iopub.status.idle":"2022-05-08T06:53:08.433747Z","shell.execute_reply.started":"2022-05-08T06:53:08.336628Z","shell.execute_reply":"2022-05-08T06:53:08.432995Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.436985Z","iopub.execute_input":"2022-05-08T06:53:08.437243Z","iopub.status.idle":"2022-05-08T06:53:08.446829Z","shell.execute_reply.started":"2022-05-08T06:53:08.437213Z","shell.execute_reply":"2022-05-08T06:53:08.445952Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"word_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n\nprint('Vocabulary = %d' % (len(vocab)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.448169Z","iopub.execute_input":"2022-05-08T06:53:08.448428Z","iopub.status.idle":"2022-05-08T06:53:08.594124Z","shell.execute_reply.started":"2022-05-08T06:53:08.448381Z","shell.execute_reply":"2022-05-08T06:53:08.592729Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"ixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\nvocab_size = len(ixtoword) + 1","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.595379Z","iopub.execute_input":"2022-05-08T06:53:08.595653Z","iopub.status.idle":"2022-05-08T06:53:08.601238Z","shell.execute_reply.started":"2022-05-08T06:53:08.595618Z","shell.execute_reply":"2022-05-08T06:53:08.600246Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"all_desc = list()\nfor key in train_descriptions.keys():\n    [all_desc.append(d) for d in train_descriptions[key]]\nlines = all_desc\nmax_length = max(len(d.split()) for d in lines)\n\nprint('Description Length: %d' % max_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.602838Z","iopub.execute_input":"2022-05-08T06:53:08.603373Z","iopub.status.idle":"2022-05-08T06:53:08.643852Z","shell.execute_reply.started":"2022-05-08T06:53:08.603336Z","shell.execute_reply":"2022-05-08T06:53:08.643225Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {} \nf = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:08.644987Z","iopub.execute_input":"2022-05-08T06:53:08.645217Z","iopub.status.idle":"2022-05-08T06:53:30.343966Z","shell.execute_reply.started":"2022-05-08T06:53:08.645185Z","shell.execute_reply":"2022-05-08T06:53:30.343200Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:30.345115Z","iopub.execute_input":"2022-05-08T06:53:30.345361Z","iopub.status.idle":"2022-05-08T06:53:30.359434Z","shell.execute_reply.started":"2022-05-08T06:53:30.345326Z","shell.execute_reply":"2022-05-08T06:53:30.358737Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Initalizing the CNN model which is used for feature extraction<br>\n**Model = InceptionNet V3**","metadata":{}},{"cell_type":"code","source":"model = InceptionV3(weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:30.360906Z","iopub.execute_input":"2022-05-08T06:53:30.361233Z","iopub.status.idle":"2022-05-08T06:53:35.225037Z","shell.execute_reply.started":"2022-05-08T06:53:30.361185Z","shell.execute_reply":"2022-05-08T06:53:35.224288Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model_new = Model(model.input, model.layers[-2].output)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:35.226516Z","iopub.execute_input":"2022-05-08T06:53:35.226774Z","iopub.status.idle":"2022-05-08T06:53:35.250287Z","shell.execute_reply.started":"2022-05-08T06:53:35.226738Z","shell.execute_reply":"2022-05-08T06:53:35.249657Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model_new.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:21:09.132691Z","iopub.execute_input":"2022-05-08T13:21:09.132961Z","iopub.status.idle":"2022-05-08T13:21:09.267044Z","shell.execute_reply.started":"2022-05-08T13:21:09.132931Z","shell.execute_reply":"2022-05-08T13:21:09.261913Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"**Since we are using Inception V-3 , we nedd to pre-process our inputs before feeding it into the model.\nHence, we need to defince a preprocess function to reshape the images(299 x 299) and feed the preprocess_input() function.**","metadata":{}},{"cell_type":"code","source":"def preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:35.251596Z","iopub.execute_input":"2022-05-08T06:53:35.251845Z","iopub.status.idle":"2022-05-08T06:53:35.256581Z","shell.execute_reply.started":"2022-05-08T06:53:35.251811Z","shell.execute_reply":"2022-05-08T06:53:35.255698Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def encode(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n    return fea_vec\n\nencoding_train = {}\nfor img in train_img:\n    encoding_train[img[len(images_path):]] = encode(img)\ntrain_features = encoding_train\n\nencoding_test = {}\nfor img in test_img:\n    encoding_test[img[len(images_path):]] = encode(img)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:53:35.257871Z","iopub.execute_input":"2022-05-08T06:53:35.258421Z","iopub.status.idle":"2022-05-08T07:00:49.905411Z","shell.execute_reply.started":"2022-05-08T06:53:35.258365Z","shell.execute_reply":"2022-05-08T07:00:49.904587Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Model for decoding i.e., LSTM","metadata":{}},{"cell_type":"code","source":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:00:49.906653Z","iopub.execute_input":"2022-05-08T07:00:49.908857Z","iopub.status.idle":"2022-05-08T07:00:50.974278Z","shell.execute_reply.started":"2022-05-08T07:00:49.908824Z","shell.execute_reply":"2022-05-08T07:00:50.973582Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:00:50.975702Z","iopub.execute_input":"2022-05-08T07:00:50.975964Z","iopub.status.idle":"2022-05-08T07:00:50.981638Z","shell.execute_reply.started":"2022-05-08T07:00:50.975931Z","shell.execute_reply":"2022-05-08T07:00:50.980950Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:00:50.982839Z","iopub.execute_input":"2022-05-08T07:00:50.983119Z","iopub.status.idle":"2022-05-08T07:00:51.000948Z","shell.execute_reply.started":"2022-05-08T07:00:50.983084Z","shell.execute_reply":"2022-05-08T07:00:51.000094Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            # retrieve the photo feature\n            photo = photos[key +'.jpg']\n            for desc in desc_list:\n                # encode the sequence\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            if n==num_photos_per_batch:\n                yield ([array(X1), array(X2)], array(y))\n                X1, X2, y = list(), list(), list()\n                n=0\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:00:51.002236Z","iopub.execute_input":"2022-05-08T07:00:51.003092Z","iopub.status.idle":"2022-05-08T07:00:51.012911Z","shell.execute_reply.started":"2022-05-08T07:00:51.003056Z","shell.execute_reply":"2022-05-08T07:00:51.012184Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"epochs = 20\nbatch_size = 3\nsteps = len(train_descriptions)//batch_size\n\ngenerator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:47:01.926348Z","iopub.execute_input":"2022-05-08T07:47:01.926899Z","iopub.status.idle":"2022-05-08T08:58:19.562987Z","shell.execute_reply.started":"2022-05-08T07:47:01.926861Z","shell.execute_reply":"2022-05-08T08:58:19.562326Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model.save('ImageCaptionPredictor1.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:54:44.650186Z","iopub.execute_input":"2022-05-08T09:54:44.650568Z","iopub.status.idle":"2022-05-08T09:54:44.712425Z","shell.execute_reply.started":"2022-05-08T09:54:44.650530Z","shell.execute_reply":"2022-05-08T09:54:44.711695Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:57:10.386411Z","iopub.execute_input":"2022-05-08T09:57:10.386985Z","iopub.status.idle":"2022-05-08T09:57:10.398434Z","shell.execute_reply.started":"2022-05-08T09:57:10.386946Z","shell.execute_reply":"2022-05-08T09:57:10.397570Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"model.trainable_variables","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","metadata":{"execution":{"iopub.status.busy":"2022-05-08T08:58:30.187844Z","iopub.execute_input":"2022-05-08T08:58:30.188100Z","iopub.status.idle":"2022-05-08T08:58:30.195093Z","shell.execute_reply.started":"2022-05-08T08:58:30.188071Z","shell.execute_reply":"2022-05-08T08:58:30.194282Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def beam_search_predictions(image, beam_index = 3):\n    start = [wordtoix[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [ixtoword[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","metadata":{"execution":{"iopub.status.busy":"2022-05-08T08:58:34.590467Z","iopub.execute_input":"2022-05-08T08:58:34.591203Z","iopub.status.idle":"2022-05-08T08:58:34.601463Z","shell.execute_reply.started":"2022-05-08T08:58:34.591162Z","shell.execute_reply":"2022-05-08T08:58:34.600670Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"pic = '2398605966_1d0c9e6a20.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\nprint(\"Greedy Search:\",greedySearch(image))\nprint(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\nprint(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\nprint(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\nprint(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T08:58:54.473034Z","iopub.execute_input":"2022-05-08T08:58:54.473428Z","iopub.status.idle":"2022-05-08T08:59:28.836476Z","shell.execute_reply.started":"2022-05-08T08:58:54.473395Z","shell.execute_reply":"2022-05-08T08:59:28.835663Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print(\"Greedy Search:\",greedySearch(image))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T08:58:40.653948Z","iopub.execute_input":"2022-05-08T08:58:40.654486Z","iopub.status.idle":"2022-05-08T08:58:41.918341Z","shell.execute_reply.started":"2022-05-08T08:58:40.654450Z","shell.execute_reply":"2022-05-08T08:58:41.917577Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T08:58:47.754194Z","iopub.execute_input":"2022-05-08T08:58:47.754480Z","iopub.status.idle":"2022-05-08T08:58:51.775999Z","shell.execute_reply.started":"2022-05-08T08:58:47.754450Z","shell.execute_reply":"2022-05-08T08:58:51.775215Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = list(encoding_test.keys())[1]\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\n\nprint(\"Greedy:\",greedySearch(image))\nprint(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\nprint(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\nprint(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:00:43.049121Z","iopub.execute_input":"2022-05-08T09:00:43.049401Z","iopub.status.idle":"2022-05-08T09:01:03.477784Z","shell.execute_reply.started":"2022-05-08T09:00:43.049356Z","shell.execute_reply":"2022-05-08T09:01:03.476278Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"doc = open(test_images_path,'r').read()\ndataset = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset.append(identifier)\n\ntest = set(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:35:49.661993Z","iopub.execute_input":"2022-05-08T09:35:49.662258Z","iopub.status.idle":"2022-05-08T09:35:49.669375Z","shell.execute_reply.started":"2022-05-08T09:35:49.662229Z","shell.execute_reply":"2022-05-08T09:35:49.668700Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"test_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in test:\n        if image_id not in test_descriptions:\n            test_descriptions[image_id] = list()\n        desc = ' '.join(image_desc)\n        test_descriptions[image_id].append(desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:51:30.852169Z","iopub.execute_input":"2022-05-08T12:51:30.852460Z","iopub.status.idle":"2022-05-08T12:51:30.914128Z","shell.execute_reply.started":"2022-05-08T12:51:30.852428Z","shell.execute_reply":"2022-05-08T12:51:30.913428Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"test_descriptions.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting for test images and finding exceptions and bias patterns","metadata":{}},{"cell_type":"code","source":"refs = []\nhyps = []\n\nimport random\nfrom tqdm import tqdm\nfor pic in tqdm(list(test_descriptions.keys())):\n    image = encoding_test[pic+'.jpg'].reshape((1,2048))\n    hyp = beam_search_predictions(image, beam_index = 7)\n    hyps.append(hyp.split())\n    tr = [caption.split() for caption in test_descriptions[pic]]\n    refs.append(tr)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:29:57.450682Z","iopub.execute_input":"2022-05-08T13:29:57.451048Z","iopub.status.idle":"2022-05-08T16:10:04.565451Z","shell.execute_reply.started":"2022-05-08T13:29:57.451011Z","shell.execute_reply":"2022-05-08T16:10:04.564233Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"refs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('refs_hyps1.pickle', 'wb') as handle:\n    pickle.dump({'refs': refs, 'hyps': hyps}, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:11:41.768328Z","iopub.execute_input":"2022-05-08T16:11:41.769009Z","iopub.status.idle":"2022-05-08T16:11:41.789240Z","shell.execute_reply.started":"2022-05-08T16:11:41.768973Z","shell.execute_reply":"2022-05-08T16:11:41.788561Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"**Getting the BLEU scores**","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\nbleu_1 = corpus_bleu(refs, hyps, weights=(1.0, 0, 0, 0))\nbleu_2 = corpus_bleu(refs, hyps, weights=(0.5, 0.5, 0, 0))\nbleu_3 = corpus_bleu(refs, hyps, weights=(0.33, 0.33, 0.33, 0))\nbleu_4 = corpus_bleu(refs, hyps, weights=(0.25, 0.25, 0.25, 0.25))\n\nprint(\"BLEU-1: {:.4f}\".format(bleu_1))\nprint(\"BLEU-2: {:.4f}\".format(bleu_2))\nprint(\"BLEU-3: {:.4f}\".format(bleu_3))\nprint(\"BLEU-4: {:.4f}\".format(bleu_4))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:11:54.971296Z","iopub.execute_input":"2022-05-08T16:11:54.971831Z","iopub.status.idle":"2022-05-08T16:11:56.302206Z","shell.execute_reply.started":"2022-05-08T16:11:54.971795Z","shell.execute_reply":"2022-05-08T16:11:56.301465Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nx = plt.imread(images_path+imgs[bleu_1_l_s[0]])\nplt.imshow(x)\nplt.show()\n\nprint(' '.join(hyps[bleu_1_l_s[0]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}